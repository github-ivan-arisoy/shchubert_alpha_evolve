"""
SchuBERT Experiments Pipeline 
================================
IMPORTANT: 
 - ‚ùó‚ùó‚ùó DON'T WORRY Ivan indeed Can provide you a .py file with the code ‚ùó‚ùó‚ùó 
 - ü§® NO .ipynb FILES WERE INVOLVED NOR INCLUDED IN HERE! ü§®

DISCLAIMER: 
- üòµ THIS FILE WILL CONTAIN A LOT OF WELL SELECTED EMOJIS AND COMMENTS GENERATED BY OPUS 4.5 TURBO 120B ü§ñ
- ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è FOR SENSITIVE LeCHAT USERS : IF IT IS TOO OVERWHELMING, PLEASE CONTACT IVAN. HE CAN REMOVE THE EMOJIS AND COMMENTS. ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
"""

import argparse
import csv
import datetime
import json
import logging
import os
import shutil
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import numpy as np
import pandas as pd
import tensorflow as tf
from keras.callbacks import EarlyStopping
from sklearn.metrics import log_loss, r2_score, roc_auc_score
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Model

# =============================================================================
# ‚öôÔ∏è  CONFIGURATION - EDIT YOUR PARAMETERS HERE  ‚öôÔ∏è
# =============================================================================
#
# This is the SINGLE SOURCE OF TRUTH for all training parameters.
# Modify values here - they will be used across all training steps.
# CLI arguments can override these defaults at runtime.
#
# =============================================================================

# -----------------------------------------------------------------------------
# üèóÔ∏è  MODEL ARCHITECTURE (shared across all steps)
# -----------------------------------------------------------------------------
MODEL_CONFIG = {
    "embed_dim": 16,              # Embedding dimension
    "ff_dim": 64,                 # Feed-forward dimension  
    "nb_head": 2,                 # Number of attention heads
    "nb_transformer_blocs": 2,    # Number of transformer blocks
}

# -----------------------------------------------------------------------------
# üìö  STEP 1: PRE-TRAINING (MLM + NT-Xent Contrastive Learning)
# -----------------------------------------------------------------------------
# Folder structure: data/pretrain/train/*.parquet (no validation for pretrain)
PRETRAIN_CONFIG = {
    # Training
    "batch_size": 1024,
    "epochs": 1,
    "learning_rate": 0.001,
    "early_stopping_patience": 3,
    "early_stopping_min_delta": 0.001,
    
    # Data (uses train/ subfolder)
    "min_seed_size": 1,
    "data_dir": "data/pretrain",
    
    # Model output
    "model_output_dir": "models/pretrain",
    "model_name": "hubert_ntxent_model",
    
    # Logs
    "logs_dir": "logs/pretrain",
}

# -----------------------------------------------------------------------------
# üéØ  STEP 2: FINE-TUNING (CTR Binary Classification)
# -----------------------------------------------------------------------------
# Folder structure:
#   data/finetune/train/*.parquet  - Training data
#   data/finetune/val/*.parquet    - Validation data (ideally: final hour of dataset)
#                                    If empty: 1% random split from train
FINETUNE_CONFIG = {
    # Training
    "batch_size": 1024,
    "epochs": 1,
    "learning_rate": 0.001,
    "early_stopping_patience": 2,
    "early_stopping_min_delta": 0.001,
    
    # Data
    "min_seed_size": 1,
    "data_dir": "data/finetune",
    "val_split_fallback": 0.01,   # 1% random split if val folder is empty
    
    # Model paths
    "pretrained_model_path": "models/pretrain/hubert_ntxent_model",
    "model_output_dir": "models/finetune",
    "model_name": "hubert_ctr_finetuned",
    
    # Logs
    "logs_dir": "logs/finetune",
}

# -----------------------------------------------------------------------------
# üßä  STEP 3: DOWNSTREAM (Frozen Backbone + New Head)
# -----------------------------------------------------------------------------
# Folder structure:
#   data/downstream/train/*.parquet - Training data
#   data/downstream/val/*.parquet   - Validation data (ideally: final hour of dataset)
#                                     If empty: 1% random split from train
#   data/downstream/test/*.parquet  - Test data for final evaluation
# 
# Test evaluation includes bias shift for negative sampling compensation
DOWNSTREAM_CONFIG = {
    # Training
    "batch_size": 15000,
    "epochs": 1,
    "learning_rate": 0.001,
    "early_stopping_patience": 2,
    "early_stopping_min_delta": 0.001,
    
    # New head architecture
    "hidden_dim": 32,             # Two Dense layers of this dimension
    
    # Data
    "min_seed_size": 1,
    "data_dir": "data/downstream",
    "val_split_fallback": 0.01,   # 1% random split if val folder is empty
    
    # Test evaluation settings
    "negative_sampling_rate": 10,  # 10% negative sampling rate for bias shift
    
    # Model paths
    "finetuned_model_path": "models/finetune/hubert_ctr_finetuned",
    "model_output_dir": "models/downstream",
    "model_name": "hubert_downstream_ctr",
    
    # Logs
    "logs_dir": "logs/downstream",
}

# -----------------------------------------------------------------------------
# üé≤  DUMMY DATA GENERATION
# -----------------------------------------------------------------------------
DUMMY_DATA_CONFIG = {
    "sequence_length": 40,
    "vocab_size": 19000,
    "ctr": 0.05,                  # Click-through rate for labels
    "pretrain_samples": 2000,
    "finetune_train_samples": 2000,
    "finetune_val_samples": 500,    # Smaller val set (ideally: final hour of dataset)
    "downstream_train_samples": 2000,
    "downstream_val_samples": 500,  # Smaller val set (ideally: final hour of dataset)
    "downstream_test_samples": 1000, # Test set for final evaluation
}

# =============================================================================
# END OF CONFIGURATION - You shouldn't need to modify anything below
# =============================================================================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


@dataclass
class PretrainParams:
    """Parameters for pre-training (MLM + NT-Xent)."""

    # Model architecture
    embed_dim: int = MODEL_CONFIG["embed_dim"]
    ff_dim: int = MODEL_CONFIG["ff_dim"]
    nb_head: int = MODEL_CONFIG["nb_head"]
    nb_transformer_blocs: int = MODEL_CONFIG["nb_transformer_blocs"]

    # Training
    batch_size: int = PRETRAIN_CONFIG["batch_size"]
    epochs: int = PRETRAIN_CONFIG["epochs"]
    learning_rate: float = PRETRAIN_CONFIG["learning_rate"]
    early_stopping_patience: int = PRETRAIN_CONFIG["early_stopping_patience"]
    early_stopping_min_delta: float = PRETRAIN_CONFIG["early_stopping_min_delta"]

    # Data
    min_seed_size: int = PRETRAIN_CONFIG["min_seed_size"]

    # Paths
    data_dir: str = PRETRAIN_CONFIG["data_dir"]
    model_output_dir: str = PRETRAIN_CONFIG["model_output_dir"]
    model_name: str = PRETRAIN_CONFIG["model_name"]
    logs_dir: str = PRETRAIN_CONFIG["logs_dir"]


@dataclass
class FinetuneParams:
    """Parameters for fine-tuning (CTR prediction)."""

    # Model architecture (must match pretrained model)
    embed_dim: int = MODEL_CONFIG["embed_dim"]
    ff_dim: int = MODEL_CONFIG["ff_dim"]
    nb_head: int = MODEL_CONFIG["nb_head"]
    nb_transformer_blocs: int = MODEL_CONFIG["nb_transformer_blocs"]

    # Training
    batch_size: int = FINETUNE_CONFIG["batch_size"]
    epochs: int = FINETUNE_CONFIG["epochs"]
    finetune_learning_rate: float = FINETUNE_CONFIG["learning_rate"]
    early_stopping_patience: int = FINETUNE_CONFIG["early_stopping_patience"]
    early_stopping_min_delta: float = FINETUNE_CONFIG["early_stopping_min_delta"]

    # Data
    min_seed_size: int = FINETUNE_CONFIG["min_seed_size"]
    val_split_fallback: float = FINETUNE_CONFIG["val_split_fallback"]

    # Paths
    data_dir: str = FINETUNE_CONFIG["data_dir"]
    pretrained_model_path: str = FINETUNE_CONFIG["pretrained_model_path"]
    model_output_dir: str = FINETUNE_CONFIG["model_output_dir"]
    model_name: str = FINETUNE_CONFIG["model_name"]
    logs_dir: str = FINETUNE_CONFIG["logs_dir"]


@dataclass
class DownstreamParams:
    """Parameters for downstream CTR training (frozen backbone + new head)."""

    # Model architecture (must match finetuned model)
    embed_dim: int = MODEL_CONFIG["embed_dim"]

    # Training
    batch_size: int = DOWNSTREAM_CONFIG["batch_size"]
    epochs: int = DOWNSTREAM_CONFIG["epochs"]
    downstream_learning_rate: float = DOWNSTREAM_CONFIG["learning_rate"]
    early_stopping_patience: int = DOWNSTREAM_CONFIG["early_stopping_patience"]
    early_stopping_min_delta: float = DOWNSTREAM_CONFIG["early_stopping_min_delta"]

    # New head architecture
    hidden_dim: int = DOWNSTREAM_CONFIG["hidden_dim"]

    # Data
    min_seed_size: int = DOWNSTREAM_CONFIG["min_seed_size"]
    val_split_fallback: float = DOWNSTREAM_CONFIG["val_split_fallback"]
    
    # Test evaluation
    negative_sampling_rate: int = DOWNSTREAM_CONFIG["negative_sampling_rate"]

    # Paths
    data_dir: str = DOWNSTREAM_CONFIG["data_dir"]
    finetuned_model_path: str = DOWNSTREAM_CONFIG["finetuned_model_path"]
    model_output_dir: str = DOWNSTREAM_CONFIG["model_output_dir"]
    model_name: str = DOWNSTREAM_CONFIG["model_name"]
    logs_dir: str = DOWNSTREAM_CONFIG["logs_dir"]


# =============================================================================
# CUSTOM LAYERS
# =============================================================================


class LinearTimeEmbedding(tf.keras.layers.Layer):
    """
    Linear transformation of continuous time values.
    Learns alpha (slope) and beta (offset) to project LN(delay) into embedding space.
    """

    def __init__(self, d_model: int, **kwargs):
        super().__init__(**kwargs)
        self.d_model = d_model
        self.alpha = self.add_weight(
            shape=(1, d_model), initializer="ones", trainable=True, name="alpha"
        )
        self.beta = self.add_weight(
            shape=(1, d_model), initializer="zeros", trainable=True, name="beta"
        )

    def call(self, time_val):
        # time_val shape: (batch, seq_len)
        time_val = tf.expand_dims(time_val, -1)  # (batch, seq_len, 1)
        embedding = tf.multiply(time_val, self.alpha) + self.beta
        return embedding

    def get_config(self):
        config = super().get_config()
        config.update({"d_model": self.d_model})
        return config


# =============================================================================
# CSV LOGGING CALLBACK
# =============================================================================


class CSVLoggingCallback(keras.callbacks.Callback):
    """
    Callback to log training and validation metrics to a CSV file at each epoch.
    
    The CSV will have columns: epoch, and all metrics from logs (e.g., loss, auc, val_loss, val_auc).
    """
    
    def __init__(self, logs_dir: str, filename: str = "training_metrics.csv"):
        super().__init__()
        self.logs_dir = Path(logs_dir)
        self.filename = filename
        self.csv_path = self.logs_dir / self.filename
        self.csv_file = None
        self.csv_writer = None
        self.keys = None
    
    def on_train_begin(self, logs=None):
        # Create logs directory if it doesn't exist
        self.logs_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"üìù Metrics will be logged to: {self.csv_path}")
    
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        
        # On first epoch, determine the keys and write header
        if self.keys is None:
            self.keys = ["epoch"] + sorted(logs.keys())
            self.csv_file = open(self.csv_path, "w", newline="")
            self.csv_writer = csv.DictWriter(self.csv_file, fieldnames=self.keys)
            self.csv_writer.writeheader()
        
        # Write the row
        row = {"epoch": epoch + 1}  # 1-indexed epochs
        row.update({k: logs.get(k, "") for k in self.keys if k != "epoch"})
        self.csv_writer.writerow(row)
        self.csv_file.flush()  # Ensure data is written
    
    def on_train_end(self, logs=None):
        if self.csv_file:
            self.csv_file.close()
            logger.info(f"‚úÖ Metrics saved to: {self.csv_path}")


# =============================================================================
# EVALUATION METRICS UTILITIES
# =============================================================================


def compute_classification_metrics(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    prefix: str = ""
) -> Dict[str, float]:
    """
    Compute classification metrics: AUC, Log-loss, R2, and RIG.
    
    Args:
        y_true: Ground truth labels (0 or 1)
        y_pred: Predicted probabilities
        prefix: Optional prefix for metric names (e.g., "val_", "test_")
    
    Returns:
        Dictionary of metric names to values
    """
    eps = np.finfo(float).eps
    
    # Clip predictions to avoid log(0)
    y_pred_clipped = np.clip(y_pred, eps, 1 - eps)
    
    # AUC
    try:
        auc = roc_auc_score(y_true, y_pred_clipped)
    except ValueError:
        auc = 0.5  # Default if only one class present
    
    # Log-loss
    ll = log_loss(y_true, y_pred_clipped, labels=[0, 1])
    
    # R2 score
    try:
        r2 = r2_score(y_true, y_pred_clipped)
    except ValueError:
        r2 = 0.0
    
    # RIG (Relative Information Gain) = (baseline_loss - model_loss) / baseline_loss
    # Baseline is predicting the mean of the training labels
    labels = y_true.copy()
    labels[labels == -1] = 0  # Handle any -1 labels
    baseline_pred = np.mean(labels)
    baseline_pred = np.clip(baseline_pred, eps, 1 - eps)
    baseline_loss = -np.mean(
        labels * np.log(baseline_pred) + (1 - labels) * np.log(1 - baseline_pred)
    )
    rig = (baseline_loss - ll) / baseline_loss if baseline_loss > 0 else 0.0
    
    metrics = {
        f"{prefix}auc": auc,
        f"{prefix}log_loss": ll,
        f"{prefix}r2": r2,
        f"{prefix}rig": rig,
    }
    
    return metrics


def shift_bias_for_negative_sampling(
    model: keras.Model,
    negative_sampling_rate: int,
    layer_name: str = "downstream_click_pred"
) -> keras.Model:
    """
    Shift the bias of the output layer to compensate for negative sampling.
    
    During training with negative sampling, the model learns a biased probability.
    This function adjusts the bias to recover the true probability:
        new_bias = old_bias + log(negative_sampling_rate / 100)
    
    Args:
        model: The trained model
        negative_sampling_rate: The negative sampling rate in percent (e.g., 10 for 10%)
        layer_name: Name of the output dense layer
    
    Returns:
        Model with adjusted bias (same model, modified in place)
    """
    try:
        last_layer = model.get_layer(layer_name)
        weights, bias = last_layer.get_weights()
        new_bias = bias + np.log(negative_sampling_rate / 100)
        last_layer.set_weights((weights, new_bias))
        logger.info(
            f"üîß Bias shifted for {negative_sampling_rate}% negative sampling: "
            f"old_bias={bias[0]:.6f}, new_bias={new_bias[0]:.6f}"
        )
    except Exception as e:
        logger.warning(f"Could not shift bias: {e}")
    
    return model


def create_clipped_model(model: keras.Model) -> keras.Model:
    """
    Create a new model with clipped outputs to prevent numerical instability.
    
    Args:
        model: Original model
    
    Returns:
        New model with outputs clipped to [1e-10, 1-1e-10]
    """
    clipped_output = tf.clip_by_value(model.outputs[0], 1e-10, 1 - 1e-10)
    clipped_model = Model(inputs=model.inputs, outputs=clipped_output)
    return clipped_model


# Valid multioutput options for RSquare metric
_VALID_MULTIOUTPUT = ("raw_values", "uniform_average", "variance_weighted")

# Type alias for acceptable dtypes
AcceptableDTypes = Optional[tf.DType]


def _reduce_average(input_tensor: tf.Tensor, weights: tf.Tensor) -> tf.Tensor:
    """Weighted average reduction for variance_weighted multioutput."""
    weights = weights / tf.reduce_sum(weights)
    return tf.reduce_sum(input_tensor * weights)


class RSquare(tf.keras.metrics.Metric):
    """Compute R^2 score (coefficient of determination).

    This metric tells how close data is to the fitted regression line.

    - Highest score can be 1.0 and it indicates that the predictors
        perfectly account for variation in the target.
    - Score 0.0 indicates that the predictors do not
        account for variation in the target.
    - It can also be negative if the model is worse than predicting the mean.

    The sample weighting for this metric implementation mimics the
    behaviour of the scikit-learn implementation of the same metric.

    Can also calculate the Adjusted R2 Score.

    Args:
        multioutput: string, the reduce method for scores.
            Should be one of ["raw_values", "uniform_average", "variance_weighted"].
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.
        num_regressors: (Optional) Number of independent regressors used (Adjusted R2).
            Defaults to zero (standard R2 score).
    """

    def __init__(
        self,
        name: str = "r2",
        dtype: AcceptableDTypes = None,
        multioutput: str = "uniform_average",
        num_regressors: int = 0,
        **kwargs,
    ):
        super().__init__(name=name, dtype=dtype, **kwargs)

        if multioutput not in _VALID_MULTIOUTPUT:
            raise ValueError(
                f"The multioutput argument must be one of {_VALID_MULTIOUTPUT}, "
                f"but was: {multioutput}"
            )
        self.multioutput = multioutput
        self.num_regressors = num_regressors
        self.num_samples = self.add_weight(name="num_samples", dtype=tf.int32)

    def update_state(self, y_true, y_pred, sample_weight=None) -> None:
        # Dynamically create weights based on y_true shape
        if not hasattr(self, "squared_sum"):
            self.squared_sum = self.add_weight(
                name="squared_sum",
                shape=y_true.shape[1:] if len(y_true.shape) > 1 else (),
                initializer="zeros",
                dtype=self._dtype,
            )
        if not hasattr(self, "sum"):
            self.sum = self.add_weight(
                name="sum",
                shape=y_true.shape[1:] if len(y_true.shape) > 1 else (),
                initializer="zeros",
                dtype=self._dtype,
            )
        if not hasattr(self, "res"):
            self.res = self.add_weight(
                name="residual",
                shape=y_true.shape[1:] if len(y_true.shape) > 1 else (),
                initializer="zeros",
                dtype=self._dtype,
            )
        if not hasattr(self, "count"):
            self.count = self.add_weight(
                name="count",
                shape=y_true.shape[1:] if len(y_true.shape) > 1 else (),
                initializer="zeros",
                dtype=self._dtype,
            )

        y_true = tf.cast(y_true, dtype=self._dtype)
        y_pred = tf.cast(y_pred, dtype=self._dtype)
        
        if sample_weight is None:
            sample_weight = tf.ones_like(y_true)
        sample_weight = tf.cast(sample_weight, dtype=self._dtype)
        
        # Broadcast weights to match y_true shape
        if len(sample_weight.shape) < len(y_true.shape):
            sample_weight = tf.expand_dims(sample_weight, -1)
        sample_weight = tf.broadcast_to(sample_weight, tf.shape(y_true))

        weighted_y_true = y_true * sample_weight
        self.sum.assign_add(tf.reduce_sum(weighted_y_true, axis=0))
        self.squared_sum.assign_add(tf.reduce_sum(y_true * weighted_y_true, axis=0))
        self.res.assign_add(tf.reduce_sum((y_true - y_pred) ** 2 * sample_weight, axis=0))
        self.count.assign_add(tf.reduce_sum(sample_weight, axis=0))
        self.num_samples.assign_add(tf.size(y_true))

    def result(self) -> tf.Tensor:
        mean = self.sum / self.count
        total = self.squared_sum - self.sum * mean
        raw_scores = 1 - (self.res / total)
        raw_scores = tf.where(tf.math.is_inf(raw_scores), 0.0, raw_scores)

        if self.multioutput == "raw_values":
            r2_score = raw_scores
        elif self.multioutput == "uniform_average":
            r2_score = tf.reduce_mean(raw_scores)
        elif self.multioutput == "variance_weighted":
            r2_score = _reduce_average(raw_scores, weights=total)
        else:
            raise RuntimeError(
                f"The multioutput attribute must be one of {_VALID_MULTIOUTPUT}, "
                f"but was: {self.multioutput}"
            )

        # Handle adjusted R2
        if self.num_regressors < 0:
            raise ValueError("num_regressors parameter should be greater than or equal to zero")

        if self.num_regressors != 0:
            n = tf.cast(self.num_samples, dtype=tf.float32)
            p = tf.cast(self.num_regressors, dtype=tf.float32)
            
            if self.num_regressors < self.num_samples - 1:
                num = tf.multiply(tf.subtract(1.0, r2_score), tf.subtract(n, 1.0))
                den = tf.subtract(tf.subtract(n, p), 1.0)
                r2_score = tf.subtract(1.0, tf.divide(num, den))

        return r2_score

    def reset_state(self) -> None:
        # Reset all variables to zeros
        if hasattr(self, "squared_sum"):
            self.squared_sum.assign(tf.zeros_like(self.squared_sum))
        if hasattr(self, "sum"):
            self.sum.assign(tf.zeros_like(self.sum))
        if hasattr(self, "res"):
            self.res.assign(tf.zeros_like(self.res))
        if hasattr(self, "count"):
            self.count.assign(tf.zeros_like(self.count))
        self.num_samples.assign(0)

    def reset_states(self):
        """Backwards compatibility alias of reset_state."""
        return self.reset_state()

    def get_config(self):
        config = {
            "multioutput": self.multioutput,
            "num_regressors": self.num_regressors,
        }
        base_config = super().get_config()
        return {**base_config, **config}


# =============================================================================
# PRE-TRAINING MODEL (MLM + NT-Xent)
# =============================================================================


class BertTrainer(tf.keras.Model):
    """
    Custom training model for pre-training with MLM + NT-Xent loss.
    Handles dual-view contrastive learning with masked language modeling.
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.loss_tracker = tf.keras.metrics.Mean(name="loss")
        self.mlm_loss_tracker = tf.keras.metrics.Mean(name="mlm_loss")
        self.contrastive_loss_tracker = tf.keras.metrics.Mean(name="contrastive_loss")

    def train_step(self, inputs):
        (
            input_a_ids,
            input_b_ids,
            input_a_times,
            input_b_times,
            mlm_labels_a,
            mlm_labels_b,
        ) = inputs

        with tf.GradientTape() as tape:
            mlm_logits_a, mlm_logits_b, cls_a, cls_b = self(
                [input_a_ids, input_b_ids, input_a_times, input_b_times], training=True
            )

            # MLM loss A
            mask_a = tf.not_equal(mlm_labels_a, -1)
            masked_logits_a = tf.boolean_mask(mlm_logits_a, mask_a)
            masked_labels_a = tf.boolean_mask(mlm_labels_a, mask_a)
            mlm_loss_a = tf.nn.sparse_softmax_cross_entropy_with_logits(
                labels=masked_labels_a, logits=masked_logits_a
            )
            mlm_loss_a = tf.reduce_mean(mlm_loss_a)

            # MLM loss B
            mask_b = tf.not_equal(mlm_labels_b, -1)
            masked_logits_b = tf.boolean_mask(mlm_logits_b, mask_b)
            masked_labels_b = tf.boolean_mask(mlm_labels_b, mask_b)
            mlm_loss_b = tf.nn.sparse_softmax_cross_entropy_with_logits(
                labels=masked_labels_b, logits=masked_logits_b
            )
            mlm_loss_b = tf.reduce_mean(mlm_loss_b)

            # Contrastive loss (NT-Xent)
            contrastive_loss = self.nt_xent_loss(cls_a, cls_b)

            # Total loss
            total_loss = mlm_loss_a + mlm_loss_b + contrastive_loss

        gradients = tape.gradient(total_loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))

        self.loss_tracker.update_state(total_loss)
        self.mlm_loss_tracker.update_state((mlm_loss_a + mlm_loss_b) / 2)
        self.contrastive_loss_tracker.update_state(contrastive_loss)

        return {
            "loss": self.loss_tracker.result(),
            "mlm_loss": self.mlm_loss_tracker.result(),
            "contrastive_loss": self.contrastive_loss_tracker.result(),
        }

    def nt_xent_loss(self, z_i, z_j, temperature: float = 0.1):
        """
        Normalized Temperature-scaled Cross Entropy Loss (NT-Xent).
        Used for contrastive learning between two views of the same user.
        """
        z_i = tf.math.l2_normalize(z_i, axis=1)
        z_j = tf.math.l2_normalize(z_j, axis=1)

        batch_size = tf.shape(z_i)[0]
        z = tf.concat([z_i, z_j], axis=0)  # (2N, D)

        similarity_matrix = tf.matmul(z, z, transpose_b=True)  # (2N, 2N)
        logits = similarity_matrix / temperature

        # Mask out self-comparisons
        diag = tf.eye(2 * batch_size)
        logits = logits * (1 - diag) - 1e9 * diag

        # Positive pair indices: [N, 2N] and [2N, N]
        positives = tf.range(batch_size, 2 * batch_size)
        positives = tf.concat([positives, tf.range(batch_size)], axis=0)

        labels = tf.cast(positives, tf.int32)
        loss = tf.keras.losses.sparse_categorical_crossentropy(
            labels, logits, from_logits=True
        )

        return tf.reduce_mean(loss)

    @property
    def metrics(self):
        return [
            self.loss_tracker,
            self.mlm_loss_tracker,
            self.contrastive_loss_tracker,
        ]


class BertFinetuneTrainer(tf.keras.Model):
    """Placeholder class for fine-tuning model compatibility when loading pretrained weights."""

    pass


# =============================================================================
# PRE-TRAINING PIPELINE
# =============================================================================


class HuBertPretrainer:
    """
    Pre-training pipeline for HuBERT using MLM + NT-Xent loss.
    Trains the foundation model to learn user history representations.
    """

    def __init__(self, params: PretrainParams):
        self.params = params
        self.input_length: Optional[int] = None
        self.time_input_length: Optional[int] = None
        self.vocab_size: Optional[int] = None

    def bert_encoder(self, query, key, value, i: int):
        """Single transformer encoder block with multi-head attention and FFN."""
        trunc_norm = tf.keras.initializers.TruncatedNormal(stddev=0.02)

        attention_output, raw_scores = layers.MultiHeadAttention(
            num_heads=self.params.nb_head,
            key_dim=self.params.embed_dim // self.params.nb_head,
            kernel_initializer=trunc_norm,
            bias_initializer="zeros",
            name=f"encoder_{i}/multiheadattention",
        )(query, key, value, return_attention_scores=True)

        attention_output = layers.Dropout(0.1, name=f"encoder_{i}/att_dropout")(
            attention_output
        )
        attention_output = layers.LayerNormalization(
            epsilon=1e-12, name=f"encoder_{i}/att_layernormalization"
        )(query + attention_output)

        ffn = keras.Sequential(
            [
                layers.Dense(
                    self.params.ff_dim,
                    kernel_initializer=trunc_norm,
                    activation="relu",
                ),
                layers.Dense(
                    self.params.embed_dim,
                    kernel_initializer=trunc_norm,
                    bias_initializer="zeros",
                ),
            ],
            name=f"encoder_{i}_ffn",
        )

        ffn_output = ffn(attention_output)
        ffn_output = layers.Dropout(0.1, name=f"encoder_{i}_ffn_dropout")(ffn_output)

        sequence_output = layers.LayerNormalization(
            epsilon=1e-12, name=f"encoder_{i}/ffn_layernormalization"
        )(attention_output + ffn_output)

        return sequence_output

    def create_bert_model(self) -> BertTrainer:
        """
        Create the BERT pre-training model with dual inputs for contrastive learning.
        Returns a model that outputs MLM logits for both views and CLS embeddings.
        """
        # Inputs for both views (A and B)
        input_a_ids = layers.Input(
            (self.input_length,), dtype=tf.int32, name="input_a_ids"
        )
        input_b_ids = layers.Input(
            (self.input_length,), dtype=tf.int32, name="input_b_ids"
        )
        input_a_times = layers.Input(
            (self.time_input_length,), dtype=tf.float32, name="input_a_times"
        )
        input_b_times = layers.Input(
            (self.time_input_length,), dtype=tf.float32, name="input_b_times"
        )

        # Shared word embedding
        word_embedding_layer = layers.Embedding(
            input_dim=self.vocab_size + 1,
            output_dim=self.params.embed_dim,
            name="word_embedding",
            mask_zero=True,
        )

        # Time embedding (shared)
        time_embedding_layer = LinearTimeEmbedding(
            d_model=self.params.embed_dim, name="time_embedding"
        )

        word_embeddings_a = word_embedding_layer(input_a_ids)
        word_embeddings_b = word_embedding_layer(input_b_ids)

        # Compute time embeddings
        time_embeddings_a = time_embedding_layer(input_a_times)
        time_embeddings_b = time_embedding_layer(input_b_times)

        # Apply mask from word embeddings to time embeddings to zero out padding
        mask_a = word_embedding_layer.compute_mask(input_a_ids)
        mask_b = word_embedding_layer.compute_mask(input_b_ids)

        mask_a_float = tf.cast(tf.expand_dims(mask_a, -1), tf.float32)
        mask_b_float = tf.cast(tf.expand_dims(mask_b, -1), tf.float32)

        time_embeddings_a = time_embeddings_a * mask_a_float
        time_embeddings_b = time_embeddings_b * mask_b_float

        # Position embedding (shared)
        position_ids = tf.range(self.input_length)
        position_embedding_layer = layers.Embedding(
            input_dim=self.input_length,
            output_dim=self.params.embed_dim,
            name="position_embedding",
            trainable=True,
        )
        position_embeddings = position_embedding_layer(position_ids)

        # Broadcast position embeddings to batch size
        def add_positional(embeddings):
            batch_size = tf.shape(embeddings)[0]
            pos = tf.tile(tf.expand_dims(position_embeddings, 0), [batch_size, 1, 1])
            return embeddings + pos

        embeddings_a = add_positional(word_embeddings_a)
        embeddings_b = add_positional(word_embeddings_b)

        # Add Time Embeddings
        embeddings_a = embeddings_a + time_embeddings_a
        embeddings_b = embeddings_b + time_embeddings_b

        # Normalization + dropout (shared)
        norm_layer = layers.LayerNormalization(epsilon=1e-12, name="emb_layernorm")
        dropout_layer = layers.Dropout(0.1, name="emb_dropout")

        embeddings_a = dropout_layer(norm_layer(embeddings_a))
        embeddings_b = dropout_layer(norm_layer(embeddings_b))

        # Define a single encoder block (shared between views)
        def build_shared_encoder():
            inputs = keras.Input(
                shape=(self.input_length, self.params.embed_dim)
            )
            x = inputs
            for i in range(self.params.nb_transformer_blocs):
                x = self.bert_encoder(x, x, x, i)
            return keras.Model(inputs=inputs, outputs=x, name="shared_encoder")

        shared_encoder = build_shared_encoder()

        # Apply shared encoder to both views
        hidden_states_a = shared_encoder(embeddings_a)
        hidden_states_b = shared_encoder(embeddings_b)

        # MLM heads
        mlm_logits_a = layers.Dense(self.vocab_size, name="mlm_logits_a")(
            hidden_states_a
        )
        mlm_logits_b = layers.Dense(self.vocab_size, name="mlm_logits_b")(
            hidden_states_b
        )

        # CLS pooling for NT-Xent
        cls_a = layers.Lambda(lambda x: x[:, 0, :], name="cls_a")(hidden_states_a)
        cls_b = layers.Lambda(lambda x: x[:, 0, :], name="cls_b")(hidden_states_b)

        # Final model
        model = BertTrainer(
            inputs=[input_a_ids, input_b_ids, input_a_times, input_b_times],
            outputs=[mlm_logits_a, mlm_logits_b, cls_a, cls_b],
            name="bert_mlm_contrastive",
        )

        optimizer = keras.optimizers.Adam(learning_rate=self.params.learning_rate)
        model.compile(optimizer=optimizer)
        logger.info(model.summary())
        return model

    def data_generator(self, df: pd.DataFrame, batch_size: int):
        """Generate batches for pre-training."""
        num_samples = len(df)

        masked_sentence_a_col = df["sentence_a"]
        masked_sentence_b_col = df["sentence_b"]
        masked_sentence_a_times_col = df["sentence_a_times"]
        masked_sentence_b_times_col = df["sentence_b_times"]
        labels_a_col = df["labels_a"]
        labels_b_col = df["labels_b"]

        for start in range(0, num_samples, batch_size):
            end = min(start + batch_size, num_samples)

            masked_sentence_a_batch = masked_sentence_a_col.iloc[start:end].tolist()
            masked_sentence_b_batch = masked_sentence_b_col.iloc[start:end].tolist()
            masked_sentence_a_times_batch = masked_sentence_a_times_col.iloc[
                start:end
            ].tolist()
            masked_sentence_b_times_batch = masked_sentence_b_times_col.iloc[
                start:end
            ].tolist()
            labels_a_batch = labels_a_col.iloc[start:end].tolist()
            labels_b_batch = labels_b_col.iloc[start:end].tolist()

            yield (
                np.array(masked_sentence_a_batch, dtype=np.int32),
                np.array(masked_sentence_b_batch, dtype=np.int32),
                np.array(masked_sentence_a_times_batch, dtype=np.float32),
                np.array(masked_sentence_b_times_batch, dtype=np.float32),
                np.array(labels_a_batch, dtype=np.int32),
                np.array(labels_b_batch, dtype=np.int32),
            )

    def _generate_tf_dataset(self, df: pd.DataFrame) -> tf.data.Dataset:
        """Create TensorFlow dataset from DataFrame."""
        ds = tf.data.Dataset.from_generator(
            lambda: self.data_generator(df, self.params.batch_size),
            output_signature=(
                tf.TensorSpec(shape=(None, self.input_length), dtype=tf.int32),
                tf.TensorSpec(shape=(None, self.input_length), dtype=tf.int32),
                tf.TensorSpec(shape=(None, self.time_input_length), dtype=tf.float32),
                tf.TensorSpec(shape=(None, self.time_input_length), dtype=tf.float32),
                tf.TensorSpec(shape=(None, self.input_length), dtype=tf.int32),
                tf.TensorSpec(shape=(None, self.input_length), dtype=tf.int32),
            ),
        )
        return ds.shuffle(1000).prefetch(tf.data.AUTOTUNE)

    def load_parquet_data(self) -> pd.DataFrame:
        """Load pre-training data from parquet files in train/ subfolder."""
        train_path = Path(self.params.data_dir) / "train"
        
        # Check train subfolder first, fall back to root for backwards compatibility
        if train_path.exists():
            data_path = train_path
        else:
            data_path = Path(self.params.data_dir)
            logger.warning(f"No train/ subfolder found, loading from {data_path}")
        
        parquet_files = list(data_path.glob("*.parquet"))

        if not parquet_files:
            raise FileNotFoundError(
                f"No parquet files found in {data_path}. "
                "Please add pre-training data files to data/pretrain/train/"
            )

        logger.info(f"Loading {len(parquet_files)} parquet file(s) from {data_path}")
        dfs = [pd.read_parquet(f) for f in parquet_files]
        data = pd.concat(dfs, ignore_index=True)

        logger.info(f"Loaded {len(data)} pre-training samples")

        if len(data) < self.params.min_seed_size:
            raise AssertionError(
                f"Pre-training data size ({len(data)}) is too small. "
                f"Minimum required: {self.params.min_seed_size}"
            )

        return data

    def generate_dataset(self) -> tf.data.Dataset:
        """Load data and create TensorFlow dataset."""
        data = self.load_parquet_data()

        # Extract metadata from first row
        self.input_length = data.pop("sequence_length").iloc[0]
        self.time_input_length = self.input_length
        self.vocab_size = data.pop("vocabulary_size").iloc[0]

        logger.info(
            f"Dataset config: input_length={self.input_length}, "
            f"vocab_size={self.vocab_size}"
        )

        return self._generate_tf_dataset(data)

    def train_model(self, model: BertTrainer, dataset: tf.data.Dataset):
        """Train the pre-training model."""
        early_stopping = EarlyStopping(
            monitor="loss",
            patience=self.params.early_stopping_patience,
            min_delta=self.params.early_stopping_min_delta,
            mode="min",
            restore_best_weights=True,
        )

        # CSV logging callback
        csv_logger = CSVLoggingCallback(
            logs_dir=self.params.logs_dir,
            filename="pretrain_metrics.csv"
        )

        # Custom callback for console logging
        class LoggingCallback(keras.callbacks.Callback):
            def on_epoch_end(self, epoch, logs=None):
                logs = logs or {}
                logger.info(
                    f"Epoch {epoch + 1}: "
                    f"loss={logs.get('loss', 0):.4f}, "
                    f"mlm_loss={logs.get('mlm_loss', 0):.4f}, "
                    f"contrastive_loss={logs.get('contrastive_loss', 0):.4f}"
                )

        _ = model.fit(
            dataset,
            verbose=2,
            epochs=self.params.epochs,
            callbacks=[early_stopping, csv_logger, LoggingCallback()],
        )
        return model

    def save_model_locally(self, model: keras.Model) -> str:
        """
        Save model to local directory.
        
        IMPORTANT: We create a standard Functional model from the BertTrainer
        to avoid serialization issues with custom Model subclasses.
        """
        output_dir = Path(self.params.model_output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        model_path = output_dir / self.params.model_name

        # Remove existing model folder if it exists (for clean save)
        if model_path.exists():
            shutil.rmtree(str(model_path))

        # Create a standard Functional model from the BertTrainer
        # This avoids serialization issues with custom Model subclasses
        functional_model = keras.Model(
            inputs=model.inputs,
            outputs=model.outputs,
            name="hubert_pretrain_functional"
        )

        # Save the Functional model (serializes properly)
        functional_model.save(str(model_path), save_format="tf")

        logger.info(f"Model saved to {model_path}")
        return str(model_path)

    def run(self) -> str:
        """Execute the full pre-training pipeline."""
        logger.info("=" * 80)
        logger.info("üöÄ HuBERT Pre-Training: MLM + NT-Xent Contrastive Learning")
        logger.info("=" * 80)

        logger.info("üìä Step 1/3: Loading pre-training dataset...")
        dataset = self.generate_dataset()

        logger.info("üß† Step 2/3: Building HuBERT pre-training model...")
        # Use MirroredStrategy for multi-GPU if available
        strategy = tf.distribute.MirroredStrategy(
            cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()
        )

        with strategy.scope():
            model = self.create_bert_model()

        logger.info("üèãÔ∏è Step 3/3: Training HuBERT model...")
        model = self.train_model(model, dataset)

        logger.info("üíæ Saving pre-trained model...")
        saved_path = self.save_model_locally(model)

        logger.info("=" * 80)
        logger.info("‚úÖ Pre-Training Complete!")
        logger.info(f"üì¶ Model saved to: {saved_path}")
        logger.info("=" * 80)

        return saved_path


# =============================================================================
# FINE-TUNING PIPELINE
# =============================================================================


class HuBertFinetuner:
    """
    Fine-tuning pipeline for HuBERT CTR prediction.
    Takes pre-trained backbone and adds a binary classification head.
    """

    def __init__(self, params: FinetuneParams):
        self.params = params
        self.input_length: Optional[int] = None
        self.vocab_size: Optional[int] = None

    def _load_pretrained_backbone(self) -> keras.Model:
        """Load the pretrained MLM + NT-Xent backbone model from local folder."""
        logger.info("Loading pretrained HuBert checkpoint...")

        model_path = Path(self.params.pretrained_model_path)
        if not model_path.exists():
            raise FileNotFoundError(
                f"Pretrained model not found at {model_path}. "
                "Run pre-training first with --mode pretrain"
            )

        # Load model directly from folder
        # Only need LinearTimeEmbedding as custom object (the model is saved as Functional)
        model = tf.keras.models.load_model(
            str(model_path),
            custom_objects={
                "LinearTimeEmbedding": LinearTimeEmbedding,
            },
            compile=False,
        )

        logger.info(f"Loaded pretrained model from {model_path}")
        return model

    def create_ctr_finetune_model(self, pretrained_model: keras.Model) -> keras.Model:
        """
        Reuse pretrained embeddings/encoder to build a single-input CTR head.
        Includes continuous time embeddings via LinearTimeEmbedding.
        """
        # Reuse layers (weights stay intact) from the pretrained HuBert backbone
        word_embedding_layer = pretrained_model.get_layer("word_embedding")
        norm_layer = pretrained_model.get_layer("emb_layernorm")
        dropout_layer = pretrained_model.get_layer("emb_dropout")
        shared_encoder = pretrained_model.get_layer("shared_encoder")

        # Reuse the LinearTimeEmbedding layer from pretrained model
        time_embedding_layer = pretrained_model.get_layer("time_embedding")

        # Inputs
        input_ids = layers.Input((self.input_length,), dtype=tf.int32, name="input_ids")
        input_times = layers.Input(
            (self.input_length,), dtype=tf.float32, name="input_times"
        )

        # Position embeddings
        position_ids = tf.range(self.input_length)
        position_embeddings = layers.Embedding(
            input_dim=self.input_length,
            output_dim=self.params.embed_dim,
            name="ctr_position_embedding",
        )(position_ids)
        batch_size = tf.shape(input_ids)[0]
        pos_broadcast = tf.tile(
            tf.expand_dims(position_embeddings, 0), [batch_size, 1, 1]
        )

        # Time embeddings (continuous)
        time_embeddings = time_embedding_layer(input_times)

        # Combine token + position + time embeddings
        embeddings = word_embedding_layer(input_ids)
        embeddings = embeddings + pos_broadcast + time_embeddings
        embeddings = dropout_layer(norm_layer(embeddings))

        encoded = shared_encoder(embeddings)
        cls_token = layers.Lambda(lambda x: x[:, 0, :], name="cls_ctr")(encoded)
        click_pred = layers.Dense(1, activation="sigmoid", name="click_ctr")(cls_token)

        model = keras.Model(
            inputs=[input_ids, input_times],
            outputs=click_pred,
            name="bert_ctr_finetune",
        )

        model.compile(
            optimizer=keras.optimizers.Adam(
                learning_rate=self.params.finetune_learning_rate
            ),
            loss="binary_crossentropy",
            metrics=[
                tf.keras.metrics.AUC(name="auc"),
                tf.keras.metrics.BinaryCrossentropy(name="log_loss"),
                RSquare(name="r2"),
            ],
        )
        logger.info(model.summary())
        return model

    def data_generator(self, df: pd.DataFrame, batch_size: int):
        """Generate batches for fine-tuning."""
        num_samples = len(df)

        sentence_a = df["sentence_a"]
        sentence_a_times = df["sentence_a_times"]
        target_click_col = df["target_click"]

        for start in range(0, num_samples, batch_size):
            end = min(start + batch_size, num_samples)

            sentence_a_batch = sentence_a.iloc[start:end].tolist()
            sentence_a_times_batch = sentence_a_times.iloc[start:end].tolist()
            target_click_batch = target_click_col.iloc[start:end].tolist()

            yield (
                {
                    "input_ids": np.array(sentence_a_batch, dtype=np.int32),
                    "input_times": np.array(sentence_a_times_batch, dtype=np.float32),
                },
                np.array(target_click_batch, dtype=np.float32),
            )

    def _generate_tf_dataset(self, df: pd.DataFrame) -> tf.data.Dataset:
        """Create TensorFlow dataset from DataFrame."""
        ds = tf.data.Dataset.from_generator(
            lambda: self.data_generator(df, self.params.batch_size),
            output_signature=(
                {
                    "input_ids": tf.TensorSpec(
                        shape=(None, self.input_length), dtype=tf.int32
                    ),
                    "input_times": tf.TensorSpec(
                        shape=(None, self.input_length), dtype=tf.float32
                    ),
                },
                tf.TensorSpec(shape=(None,), dtype=tf.float32),
            ),
        )
        return ds.shuffle(1000).prefetch(tf.data.AUTOTUNE)

    def load_parquet_data_from_folder(self, folder_path: Path) -> pd.DataFrame:
        """Load all parquet files from a folder."""
        parquet_files = list(folder_path.glob("*.parquet"))
        if not parquet_files:
            return None
        dfs = [pd.read_parquet(f) for f in parquet_files]
        return pd.concat(dfs, ignore_index=True)

    def load_train_val_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Load training and validation data from train/ and val/ subfolders.
        
        If val/ folder is empty or missing, performs a random 1% split from training data.
        Ideally, validation should be the final hour of the dataset.
        
        Returns:
            Tuple of (train_df, val_df)
        """
        base_path = Path(self.params.data_dir)
        train_path = base_path / "train"
        val_path = base_path / "val"
        
        # Load training data
        if train_path.exists():
            train_data = self.load_parquet_data_from_folder(train_path)
            train_source = train_path
        else:
            # Fallback to root folder
            train_data = self.load_parquet_data_from_folder(base_path)
            train_source = base_path
            logger.warning(f"No train/ subfolder found, loading from {base_path}")
        
        if train_data is None or len(train_data) == 0:
            raise FileNotFoundError(
                f"No training parquet files found in {train_source}. "
                "Please add fine-tuning data files to data/finetune/train/"
            )
        
        logger.info(
            f"Loaded {len(train_data)} training samples from {train_source} "
            f"(clicks: {train_data['target_click'].sum()}, "
            f"CTR: {train_data['target_click'].mean():.4f})"
        )
        
        # Load validation data
        val_data = None
        if val_path.exists():
            val_data = self.load_parquet_data_from_folder(val_path)
        
        if val_data is None or len(val_data) == 0:
            # Perform random 1% split
            split_ratio = self.params.val_split_fallback
            logger.warning(
                f"‚ö†Ô∏è No validation data found in val/ folder. "
                f"Performing random {split_ratio*100:.1f}% split from training data. "
                f"(Ideally, use the final hour of the dataset as validation)"
            )
            
            # Shuffle and split
            train_data = train_data.sample(frac=1, random_state=42).reset_index(drop=True)
            split_idx = int(len(train_data) * (1 - split_ratio))
            val_data = train_data.iloc[split_idx:].reset_index(drop=True)
            train_data = train_data.iloc[:split_idx].reset_index(drop=True)
            
            logger.info(
                f"Split result: {len(train_data)} train, {len(val_data)} val samples"
            )
        else:
            logger.info(
                f"Loaded {len(val_data)} validation samples from {val_path} "
                f"(clicks: {val_data['target_click'].sum()}, "
                f"CTR: {val_data['target_click'].mean():.4f})"
            )
        
        return train_data, val_data

    def generate_dataset_from_df(self, data: pd.DataFrame) -> tf.data.Dataset:
        """Create TensorFlow dataset from DataFrame, extracting metadata."""
        # Extract metadata from first row (copy to avoid modifying original)
        data = data.copy()
        self.input_length = data.pop("sequence_length").iloc[0]
        self.vocab_size = data.pop("vocabulary_size").iloc[0]

        logger.info(
            f"Dataset config: input_length={self.input_length}, "
            f"vocab_size={self.vocab_size}"
        )

        return self._generate_tf_dataset(data)

    def generate_datasets(self) -> Tuple[tf.data.Dataset, tf.data.Dataset]:
        """
        Load train and validation data and create TensorFlow datasets.
        
        Returns:
            Tuple of (train_dataset, val_dataset)
        """
        train_data, val_data = self.load_train_val_data()
        
        train_dataset = self.generate_dataset_from_df(train_data)
        val_dataset = self.generate_dataset_from_df(val_data)
        
        return train_dataset, val_dataset

    def train_model(
        self,
        model: keras.Model,
        train_dataset: tf.data.Dataset,
        val_dataset: Optional[tf.data.Dataset] = None,
    ) -> keras.Model:
        """Train the fine-tuning model."""
        callbacks = []

        if val_dataset is not None:
            early_stopping = EarlyStopping(
                monitor="val_loss",
                patience=self.params.early_stopping_patience,
                min_delta=self.params.early_stopping_min_delta,
                mode="min",
                restore_best_weights=True,
                verbose=1,
            )
            callbacks.append(early_stopping)
            logger.info("Early stopping enabled on val_loss")
        else:
            early_stopping = EarlyStopping(
                monitor="loss",
                patience=self.params.early_stopping_patience,
                min_delta=self.params.early_stopping_min_delta,
                mode="min",
                restore_best_weights=True,
                verbose=1,
            )
            callbacks.append(early_stopping)
            logger.info("Early stopping enabled on loss (no validation set)")

        # CSV logging callback
        csv_logger = CSVLoggingCallback(
            logs_dir=self.params.logs_dir,
            filename="finetune_metrics.csv"
        )
        callbacks.append(csv_logger)

        # Custom callback for console logging
        class LoggingCallback(keras.callbacks.Callback):
            def on_epoch_end(self, epoch, logs=None):
                logs = logs or {}
                train_str = (
                    f"loss={logs.get('loss', 0):.4f}, "
                    f"auc={logs.get('auc', 0):.4f}, "
                    f"log_loss={logs.get('log_loss', 0):.4f}, "
                    f"r2={logs.get('r2', 0):.4f}"
                )
                val_str = ""
                if "val_loss" in logs:
                    val_str = (
                        f" | val: loss={logs.get('val_loss', 0):.4f}, "
                        f"auc={logs.get('val_auc', 0):.4f}, "
                        f"log_loss={logs.get('val_log_loss', 0):.4f}, "
                        f"r2={logs.get('val_r2', 0):.4f}"
                    )
                logger.info(f"Epoch {epoch + 1}: {train_str}{val_str}")

        callbacks.append(LoggingCallback())

        _ = model.fit(
            train_dataset,
            validation_data=val_dataset,
            verbose=2,
            epochs=self.params.epochs,
            callbacks=callbacks,
        )
        return model

    def save_model_locally(self, model: keras.Model) -> str:
        output_dir = Path(self.params.model_output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        model_path = output_dir / self.params.model_name

        # Remove existing model folder if it exists (for clean save)
        if model_path.exists():
            shutil.rmtree(str(model_path))

        # Save model directly as folder
        model.save(str(model_path), save_format="tf")

        logger.info(f"Model saved to {model_path}")
        return str(model_path)

    def run(self) -> str:
        """Execute the full fine-tuning pipeline."""
        logger.info("=" * 80)
        logger.info("üöÄ HuBERT Fine-Tuning: CTR Binary Classification")
        logger.info("=" * 80)

        logger.info("üìä Step 1/4: Loading training and validation datasets...")
        train_dataset, val_dataset = self.generate_datasets()

        logger.info("üß† Step 2/4: Loading pretrained backbone and building CTR head...")
        strategy = tf.distribute.MirroredStrategy(
            cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()
        )

        with strategy.scope():
            pretrained_model = self._load_pretrained_backbone()
            model = self.create_ctr_finetune_model(pretrained_model)

        logger.info("üèãÔ∏è Step 3/4: Training CTR HuBert Model...")
        model = self.train_model(model, train_dataset, val_dataset)

        logger.info("üíæ Step 4/4: Saving fine-tuned model...")
        saved_path = self.save_model_locally(model)

        logger.info("=" * 80)
        logger.info("‚úÖ Fine-Tuning Complete!")
        logger.info(f"üì¶ Model saved to: {saved_path}")
        logger.info("=" * 80)

        return saved_path


# =============================================================================
# DOWNSTREAM PIPELINE (Frozen Backbone + New Head)
# =============================================================================


class HuBertDownstream:
    """
    Downstream CTR training pipeline.
    
    Takes the fine-tuned model, removes the CTR head, freezes the backbone,
    and adds two new Dense(32) layers + sigmoid for training.
    Only the new layers are trained.
    """

    def __init__(self, params: DownstreamParams):
        self.params = params
        self.input_length: Optional[int] = None
        self.vocab_size: Optional[int] = None

    def _load_finetuned_model(self) -> keras.Model:
        """Load the fine-tuned CTR model from local folder."""
        logger.info("Loading fine-tuned HuBert model...")

        model_path = Path(self.params.finetuned_model_path)
        if not model_path.exists():
            raise FileNotFoundError(
                f"Fine-tuned model not found at {model_path}. "
                "Run fine-tuning first with --mode finetune"
            )

        # Load model directly from folder
        model = tf.keras.models.load_model(
            str(model_path),
            custom_objects={
                "LinearTimeEmbedding": LinearTimeEmbedding,
            },
            compile=False,
        )

        logger.info(f"Loaded fine-tuned model from {model_path}")
        return model

    def create_downstream_model(self, finetuned_model: keras.Model) -> keras.Model:
        """
        Create downstream model:
        1. Take fine-tuned model
        2. Remove the CTR prediction head (click_ctr layer)
        3. Freeze all backbone layers
        4. Add two Dense(32) layers + sigmoid output
        5. Only train the new layers
        """
        # Get the CLS token output (before the click_ctr head)
        # The fine-tuned model has: inputs -> ... -> cls_ctr -> click_ctr
        cls_layer = finetuned_model.get_layer("cls_ctr")
        cls_output = cls_layer.output

        # Create a new model that outputs the CLS embedding (frozen backbone)
        backbone_model = keras.Model(
            inputs=finetuned_model.inputs,
            outputs=cls_output,
            name="frozen_backbone"
        )

        for layer in backbone_model.layers:
            layer.trainable = False

        logger.info(f"Froze {len(backbone_model.layers)} backbone layers")

        input_ids = layers.Input(
            (self.input_length,), dtype=tf.int32, name="input_ids"
        )
        input_times = layers.Input(
            (self.input_length,), dtype=tf.float32, name="input_times"
        )

        # Get CLS embedding from frozen backbone
        cls_embedding = backbone_model([input_ids, input_times])

        # New trainable head: Dense(32) -> Dense(32) -> Sigmoid
        x = layers.Dense(
            self.params.hidden_dim,
            activation="relu",
            name="downstream_dense_1"
        )(cls_embedding)
        x = layers.Dense(
            self.params.hidden_dim,
            activation="relu",
            name="downstream_dense_2"
        )(x)
        click_pred = layers.Dense(
            1,
            activation="sigmoid",
            name="downstream_click_pred"
        )(x)

        # Create final model
        model = keras.Model(
            inputs=[input_ids, input_times],
            outputs=click_pred,
            name="hubert_downstream_ctr"
        )

        # Compile with Adam optimizer
        model.compile(
            optimizer=keras.optimizers.Adam(
                learning_rate=self.params.downstream_learning_rate
            ),
            loss="binary_crossentropy",
            metrics=[
                tf.keras.metrics.AUC(name="auc"),
                tf.keras.metrics.BinaryCrossentropy(name="log_loss"),
                RSquare(name="r2"),
            ],
        )

        # Log trainable vs non-trainable params
        trainable_count = sum(
            tf.keras.backend.count_params(w) for w in model.trainable_weights
        )
        non_trainable_count = sum(
            tf.keras.backend.count_params(w) for w in model.non_trainable_weights
        )
        logger.info(
            f"Model params: {trainable_count:,} trainable, "
            f"{non_trainable_count:,} frozen"
        )
        logger.info(model.summary())

        return model

    def data_generator(self, df: pd.DataFrame, batch_size: int):
        """Generate batches for downstream training (same format as finetune)."""
        num_samples = len(df)

        sentence_a = df["sentence_a"]
        sentence_a_times = df["sentence_a_times"]
        target_click_col = df["target_click"]

        for start in range(0, num_samples, batch_size):
            end = min(start + batch_size, num_samples)

            sentence_a_batch = sentence_a.iloc[start:end].tolist()
            sentence_a_times_batch = sentence_a_times.iloc[start:end].tolist()
            target_click_batch = target_click_col.iloc[start:end].tolist()

            yield (
                {
                    "input_ids": np.array(sentence_a_batch, dtype=np.int32),
                    "input_times": np.array(sentence_a_times_batch, dtype=np.float32),
                },
                np.array(target_click_batch, dtype=np.float32),
            )

    def _generate_tf_dataset(self, df: pd.DataFrame) -> tf.data.Dataset:
        """Create TensorFlow dataset from DataFrame."""
        ds = tf.data.Dataset.from_generator(
            lambda: self.data_generator(df, self.params.batch_size),
            output_signature=(
                {
                    "input_ids": tf.TensorSpec(
                        shape=(None, self.input_length), dtype=tf.int32
                    ),
                    "input_times": tf.TensorSpec(
                        shape=(None, self.input_length), dtype=tf.float32
                    ),
                },
                tf.TensorSpec(shape=(None,), dtype=tf.float32),
            ),
        )
        return ds.shuffle(1000).prefetch(tf.data.AUTOTUNE)

    def load_parquet_data_from_folder(self, folder_path: Path) -> Optional[pd.DataFrame]:
        """Load all parquet files from a folder."""
        parquet_files = list(folder_path.glob("*.parquet"))
        if not parquet_files:
            return None
        dfs = [pd.read_parquet(f) for f in parquet_files]
        return pd.concat(dfs, ignore_index=True)

    def load_train_val_test_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:
        """
        Load training, validation, and test data from subfolders.
        
        Folder structure:
            data/downstream/train/*.parquet - Training data
            data/downstream/val/*.parquet   - Validation data (fallback: 1% random split)
            data/downstream/test/*.parquet  - Test data for final evaluation
        
        Returns:
            Tuple of (train_df, val_df, test_df) - test_df can be None if not provided
        """
        base_path = Path(self.params.data_dir)
        train_path = base_path / "train"
        val_path = base_path / "val"
        test_path = base_path / "test"
        
        # Load training data
        if train_path.exists():
            train_data = self.load_parquet_data_from_folder(train_path)
            train_source = train_path
        else:
            train_data = self.load_parquet_data_from_folder(base_path)
            train_source = base_path
            logger.warning(f"No train/ subfolder found, loading from {base_path}")
        
        if train_data is None or len(train_data) == 0:
            raise FileNotFoundError(
                f"No training parquet files found in {train_source}. "
                "Please add downstream data files to data/downstream/train/"
            )
        
        logger.info(
            f"Loaded {len(train_data)} training samples from {train_source} "
            f"(clicks: {train_data['target_click'].sum()}, "
            f"CTR: {train_data['target_click'].mean():.4f})"
        )
        
        # Load validation data
        val_data = None
        if val_path.exists():
            val_data = self.load_parquet_data_from_folder(val_path)
        
        if val_data is None or len(val_data) == 0:
            # Perform random 1% split
            split_ratio = self.params.val_split_fallback
            logger.warning(
                f"‚ö†Ô∏è No validation data found in val/ folder. "
                f"Performing random {split_ratio*100:.1f}% split from training data. "
                f"(Ideally, use the final hour of the dataset as validation)"
            )
            
            train_data = train_data.sample(frac=1, random_state=42).reset_index(drop=True)
            split_idx = int(len(train_data) * (1 - split_ratio))
            val_data = train_data.iloc[split_idx:].reset_index(drop=True)
            train_data = train_data.iloc[:split_idx].reset_index(drop=True)
            
            logger.info(
                f"Split result: {len(train_data)} train, {len(val_data)} val samples"
            )
        else:
            logger.info(
                f"Loaded {len(val_data)} validation samples from {val_path} "
                f"(clicks: {val_data['target_click'].sum()}, "
                f"CTR: {val_data['target_click'].mean():.4f})"
            )
        
        # Load test data (optional)
        test_data = None
        if test_path.exists():
            test_data = self.load_parquet_data_from_folder(test_path)
            if test_data is not None and len(test_data) > 0:
                logger.info(
                    f"Loaded {len(test_data)} test samples from {test_path} "
                    f"(clicks: {test_data['target_click'].sum()}, "
                    f"CTR: {test_data['target_click'].mean():.4f})"
                )
            else:
                test_data = None
                logger.warning("No test data found in test/ folder. Skipping test evaluation.")
        else:
            logger.warning("No test/ folder found. Skipping test evaluation.")
        
        return train_data, val_data, test_data

    def generate_dataset_from_df(self, data: pd.DataFrame) -> tf.data.Dataset:
        """Create TensorFlow dataset from DataFrame, extracting metadata."""
        data = data.copy()
        self.input_length = data.pop("sequence_length").iloc[0]
        self.vocab_size = data.pop("vocabulary_size").iloc[0]

        logger.info(
            f"Dataset config: input_length={self.input_length}, "
            f"vocab_size={self.vocab_size}"
        )

        return self._generate_tf_dataset(data)

    def generate_datasets(self) -> Tuple[tf.data.Dataset, tf.data.Dataset, Optional[Tuple[np.ndarray, np.ndarray]]]:
        """
        Load train, val, and test data and create TensorFlow datasets.
        
        Returns:
            Tuple of (train_dataset, val_dataset, test_arrays)
            where test_arrays is (X_test, y_test) numpy arrays or None
        """
        train_data, val_data, test_data = self.load_train_val_test_data()
        
        train_dataset = self.generate_dataset_from_df(train_data)
        val_dataset = self.generate_dataset_from_df(val_data)
        
        # For test data, we need numpy arrays for evaluation (not TF dataset)
        test_arrays = None
        if test_data is not None:
            test_data = test_data.copy()
            _ = test_data.pop("sequence_length")
            _ = test_data.pop("vocabulary_size")
            
            # Handle both column naming conventions (sentence_a or input_a)
            input_col = "sentence_a" if "sentence_a" in test_data.columns else "input_a"
            time_col = "sentence_a_times" if "sentence_a_times" in test_data.columns else "input_a_times"
            
            # Prepare X_test (dict of numpy arrays for model input)
            X_test = {
                "input_ids": np.stack(test_data[input_col].values),
                "input_times": np.stack(test_data[time_col].values) if time_col in test_data.columns else np.zeros((len(test_data), self.input_length), dtype=np.float32),
            }
            y_test = test_data["target_click"].values.astype(np.float32)
            test_arrays = (X_test, y_test)
        
        return train_dataset, val_dataset, test_arrays

    def train_model(
        self,
        model: keras.Model,
        train_dataset: tf.data.Dataset,
        val_dataset: Optional[tf.data.Dataset] = None,
    ) -> keras.Model:
        """Train the downstream model (only new layers are trainable)."""
        callbacks = []

        if val_dataset is not None:
            early_stopping = EarlyStopping(
                monitor="val_loss",
                patience=self.params.early_stopping_patience,
                min_delta=self.params.early_stopping_min_delta,
                mode="min",
                restore_best_weights=True,
                verbose=1,
            )
            callbacks.append(early_stopping)
            logger.info("Early stopping enabled on val_loss")
        else:
            early_stopping = EarlyStopping(
                monitor="loss",
                patience=self.params.early_stopping_patience,
                min_delta=self.params.early_stopping_min_delta,
                mode="min",
                restore_best_weights=True,
                verbose=1,
            )
            callbacks.append(early_stopping)
            logger.info("Early stopping enabled on loss (no validation set)")

        # CSV logging callback
        csv_logger = CSVLoggingCallback(
            logs_dir=self.params.logs_dir,
            filename="downstream_metrics.csv"
        )
        callbacks.append(csv_logger)

        # Custom callback for console logging
        class LoggingCallback(keras.callbacks.Callback):
            def on_epoch_end(self, epoch, logs=None):
                logs = logs or {}
                train_str = (
                    f"loss={logs.get('loss', 0):.4f}, "
                    f"auc={logs.get('auc', 0):.4f}, "
                    f"log_loss={logs.get('log_loss', 0):.4f}, "
                    f"r2={logs.get('r2', 0):.4f}"
                )
                val_str = ""
                if "val_loss" in logs:
                    val_str = (
                        f" | val: loss={logs.get('val_loss', 0):.4f}, "
                        f"auc={logs.get('val_auc', 0):.4f}, "
                        f"log_loss={logs.get('val_log_loss', 0):.4f}, "
                        f"r2={logs.get('val_r2', 0):.4f}"
                    )
                logger.info(f"Epoch {epoch + 1}: {train_str}{val_str}")

        callbacks.append(LoggingCallback())

        _ = model.fit(
            train_dataset,
            validation_data=val_dataset,
            verbose=2,
            epochs=self.params.epochs,
            callbacks=callbacks,
        )
        return model

    def save_model_locally(self, model: keras.Model) -> str:
        output_dir = Path(self.params.model_output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        model_path = output_dir / self.params.model_name

        # Remove existing model folder if it exists (for clean save)
        if model_path.exists():
            shutil.rmtree(str(model_path))

        # Save model directly as folder
        model.save(str(model_path), save_format="tf")

        logger.info(f"Model saved to {model_path}")
        return str(model_path)

    def evaluate_on_test(
        self,
        model: keras.Model,
        test_arrays: Tuple[Dict[str, np.ndarray], np.ndarray],
    ) -> Dict[str, float]:
        """
        Evaluate the model on test data with bias shift for negative sampling.
        
        Args:
            model: Trained downstream model
            test_arrays: Tuple of (X_test dict, y_test array)
        
        Returns:
            Dictionary of test metrics
        """
        X_test, y_test = test_arrays
        
        logger.info("=" * 60)
        logger.info("üìä Test Evaluation with Bias Shift")
        logger.info("=" * 60)
        
        # Apply bias shift for negative sampling
        logger.info(
            f"Applying bias shift for {self.params.negative_sampling_rate}% "
            "negative sampling rate..."
        )
        model = shift_bias_for_negative_sampling(
            model,
            self.params.negative_sampling_rate,
            layer_name="downstream_click_pred"
        )
        
        # Create clipped model for numerical stability
        clipped_model = create_clipped_model(model)
        
        # Predict
        logger.info(f"Running predictions on {len(y_test)} test samples...")
        y_pred = clipped_model.predict(
            X_test,
            batch_size=self.params.batch_size,
            verbose=0
        ).flatten()
        
        # Compute metrics
        metrics = compute_classification_metrics(y_test, y_pred, prefix="test_")
        
        # Log metrics
        logger.info("-" * 60)
        logger.info("üìà Test Metrics:")
        logger.info(f"   AUC:      {metrics['test_auc']:.6f}")
        logger.info(f"   Log-loss: {metrics['test_log_loss']:.6f}")
        logger.info(f"   R¬≤:       {metrics['test_r2']:.6f}")
        logger.info(f"   RIG:      {metrics['test_rig']:.6f}")
        logger.info("-" * 60)
        
        # Save test metrics to CSV
        test_metrics_path = Path(self.params.logs_dir) / "test_metrics.csv"
        test_metrics_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(test_metrics_path, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=sorted(metrics.keys()))
            writer.writeheader()
            writer.writerow(metrics)
        
        logger.info(f"‚úÖ Test metrics saved to: {test_metrics_path}")
        
        return metrics

    def run(self) -> str:
        """Execute the full downstream training pipeline."""
        logger.info("=" * 80)
        logger.info("üöÄ HuBERT Downstream: Frozen Backbone + New CTR Head")
        logger.info("=" * 80)

        logger.info("üìä Step 1/6: Loading training, validation, and test datasets...")
        train_dataset, val_dataset, test_arrays = self.generate_datasets()

        logger.info("üß† Step 2/6: Loading fine-tuned model and creating downstream head...")
        strategy = tf.distribute.MirroredStrategy(
            cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()
        )

        with strategy.scope():
            finetuned_model = self._load_finetuned_model()
            model = self.create_downstream_model(finetuned_model)

        logger.info("üèãÔ∏è Step 3/6: Training Downstream CTR Model (only new layers)...")
        model = self.train_model(model, train_dataset, val_dataset)

        logger.info("üíæ Step 4/6: Saving downstream model...")
        saved_path = self.save_model_locally(model)

        # Test evaluation (if test data provided)
        if test_arrays is not None:
            logger.info("üìä Step 5/6: Evaluating on test set...")
            test_metrics = self.evaluate_on_test(model, test_arrays)
        else:
            logger.info("‚è≠Ô∏è Step 5/6: Skipping test evaluation (no test data)")
            test_metrics = None

        logger.info("=" * 80)
        logger.info("‚úÖ Downstream Training Complete!")
        logger.info(f"üì¶ Model saved to: {saved_path}")
        if test_metrics:
            logger.info(f"üìä Test AUC: {test_metrics['test_auc']:.4f}, RIG: {test_metrics['test_rig']:.4f}")
        logger.info("=" * 80)

        return saved_path


# =============================================================================
# DUMMY DATA GENERATION (for testing)
# =============================================================================


def generate_dummy_pretrain_data(
    output_path: str = "data/pretrain/train/dummy_pretrain.parquet",
    num_samples: int = DUMMY_DATA_CONFIG["pretrain_samples"],
    sequence_length: int = DUMMY_DATA_CONFIG["sequence_length"],
    vocab_size: int = DUMMY_DATA_CONFIG["vocab_size"],
):
    """Generate dummy pre-training data for testing (stored in train/ subfolder)."""
    logger.info(f"Generating dummy pre-training data with {num_samples} samples...")

    np.random.seed(42)

    # Generate random sequences
    sentence_a = [
        np.random.randint(1, vocab_size, size=sequence_length).tolist()
        for _ in range(num_samples)
    ]
    sentence_b = [
        np.random.randint(1, vocab_size, size=sequence_length).tolist()
        for _ in range(num_samples)
    ]

    # Generate time values (log-scaled delays)
    sentence_a_times = [
        np.random.exponential(1.0, size=sequence_length).tolist()
        for _ in range(num_samples)
    ]
    sentence_b_times = [
        np.random.exponential(1.0, size=sequence_length).tolist()
        for _ in range(num_samples)
    ]

    # Generate MLM labels (-1 for non-masked, token_id for masked positions)
    def generate_mlm_labels(sentence):
        labels = np.full(sequence_length, -1, dtype=np.int32)
        # Mask ~15% of tokens
        mask_indices = np.random.choice(
            sequence_length, size=int(sequence_length * 0.15), replace=False
        )
        for idx in mask_indices:
            labels[idx] = sentence[idx]
        return labels.tolist()

    labels_a = [generate_mlm_labels(s) for s in sentence_a]
    labels_b = [generate_mlm_labels(s) for s in sentence_b]

    df = pd.DataFrame(
        {
            "sentence_a": sentence_a,
            "sentence_b": sentence_b,
            "sentence_a_times": sentence_a_times,
            "sentence_b_times": sentence_b_times,
            "labels_a": labels_a,
            "labels_b": labels_b,
            "sequence_length": [sequence_length] * num_samples,
            "vocabulary_size": [vocab_size] * num_samples,
        }
    )

    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(output_path, index=False)
    logger.info(f"Dummy pre-training data saved to {output_path}")


def generate_dummy_finetune_data(
    output_path: str = "data/finetune/train/dummy_finetune_train.parquet",
    num_samples: int = DUMMY_DATA_CONFIG["finetune_train_samples"],
    sequence_length: int = DUMMY_DATA_CONFIG["sequence_length"],
    vocab_size: int = DUMMY_DATA_CONFIG["vocab_size"],
    ctr: float = DUMMY_DATA_CONFIG["ctr"],
):
    """Generate dummy fine-tuning data for testing (stored in train/ or val/ subfolder)."""
    logger.info(f"Generating dummy fine-tuning data with {num_samples} samples...")

    np.random.seed(42)

    # Generate random sequences
    sentence_a = [
        np.random.randint(1, vocab_size, size=sequence_length).tolist()
        for _ in range(num_samples)
    ]

    # Generate time values
    sentence_a_times = [
        np.random.exponential(1.0, size=sequence_length).tolist()
        for _ in range(num_samples)
    ]

    # Generate binary click labels with specified CTR
    target_click = np.random.binomial(1, ctr, size=num_samples).tolist()

    df = pd.DataFrame(
        {
            "sentence_a": sentence_a,
            "sentence_a_times": sentence_a_times,
            "target_click": target_click,
            "sequence_length": [sequence_length] * num_samples,
            "vocabulary_size": [vocab_size] * num_samples,
        }
    )

    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(output_path, index=False)
    logger.info(f"Dummy fine-tuning data saved to {output_path}")


def generate_dummy_downstream_data(
    output_path: str = "data/downstream/train/dummy_downstream_train.parquet",
    num_samples: int = DUMMY_DATA_CONFIG["downstream_train_samples"],
    sequence_length: int = DUMMY_DATA_CONFIG["sequence_length"],
    vocab_size: int = DUMMY_DATA_CONFIG["vocab_size"],
    ctr: float = DUMMY_DATA_CONFIG["ctr"],
):
    """
    Generate dummy downstream data for testing.
    Same format as fine-tuning: sentence_a, sentence_a_times, target_click.
    """
    logger.info(f"Generating dummy downstream data with {num_samples} samples...")

    np.random.seed(123)  # Different seed for downstream data

    # Generate random sequences
    sentence_a = [
        np.random.randint(1, vocab_size, size=sequence_length).tolist()
        for _ in range(num_samples)
    ]

    # Generate time values
    sentence_a_times = [
        np.random.exponential(1.0, size=sequence_length).tolist()
        for _ in range(num_samples)
    ]

    # Generate binary click labels with specified CTR
    target_click = np.random.binomial(1, ctr, size=num_samples).tolist()

    df = pd.DataFrame(
        {
            "sentence_a": sentence_a,
            "sentence_a_times": sentence_a_times,
            "target_click": target_click,
            "sequence_length": [sequence_length] * num_samples,
            "vocabulary_size": [vocab_size] * num_samples,
        }
    )

    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(output_path, index=False)
    logger.info(f"Dummy downstream data saved to {output_path}")


# =============================================================================
# MAIN ENTRY POINT
# =============================================================================


def main():
    parser = argparse.ArgumentParser(
        description="HuBERT Experiment Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Run full pipeline (pre-training + fine-tuning + downstream)
    python hubert_experiment.py --mode all

    # Run only pre-training
    python hubert_experiment.py --mode pretrain

    # Run only fine-tuning (requires pretrained model)
    python hubert_experiment.py --mode finetune

    # Run only downstream (requires fine-tuned model)
    python hubert_experiment.py --mode downstream

    # Generate dummy data for testing
    python hubert_experiment.py --mode generate-dummy

    # Custom parameters
    python hubert_experiment.py --mode all --epochs 5 --batch-size 128
        """,
    )

    parser.add_argument(
        "--mode",
        type=str,
        choices=["pretrain", "finetune", "downstream", "all", "generate-dummy"],
        default="all",
        help="Training mode: pretrain, finetune, downstream, all, or generate-dummy",
    )

    # Model architecture (uses MODEL_CONFIG defaults)
    parser.add_argument(
        "--embed-dim", type=int, default=MODEL_CONFIG["embed_dim"],
        help="Embedding dimension"
    )
    parser.add_argument(
        "--ff-dim", type=int, default=MODEL_CONFIG["ff_dim"],
        help="Feed-forward dimension"
    )
    parser.add_argument(
        "--nb-head", type=int, default=MODEL_CONFIG["nb_head"],
        help="Number of attention heads"
    )
    parser.add_argument(
        "--nb-transformer-blocs", type=int, default=MODEL_CONFIG["nb_transformer_blocs"],
        help="Number of transformer blocks",
    )

    # Training parameters (uses step-specific config defaults)
    parser.add_argument(
        "--batch-size", type=int, default=PRETRAIN_CONFIG["batch_size"],
        help="Batch size for pretrain/finetune"
    )
    parser.add_argument(
        "--epochs", type=int, default=PRETRAIN_CONFIG["epochs"],
        help="Number of epochs"
    )
    parser.add_argument(
        "--learning-rate", type=float, default=PRETRAIN_CONFIG["learning_rate"],
        help="Learning rate for pretrain"
    )
    parser.add_argument(
        "--finetune-learning-rate", type=float, default=FINETUNE_CONFIG["learning_rate"],
        help="Learning rate for finetuning",
    )

    # Data paths (uses step-specific config defaults)
    parser.add_argument(
        "--pretrain-data-dir", type=str, default=PRETRAIN_CONFIG["data_dir"],
        help="Pre-training data directory",
    )
    parser.add_argument(
        "--finetune-data-dir", type=str, default=FINETUNE_CONFIG["data_dir"],
        help="Fine-tuning data directory",
    )

    # Model paths (uses step-specific config defaults)
    parser.add_argument(
        "--pretrain-model-dir", type=str, default=PRETRAIN_CONFIG["model_output_dir"],
        help="Pre-trained model output directory",
    )
    parser.add_argument(
        "--finetune-model-dir", type=str, default=FINETUNE_CONFIG["model_output_dir"],
        help="Fine-tuned model output directory",
    )
    parser.add_argument(
        "--downstream-data-dir", type=str, default=DOWNSTREAM_CONFIG["data_dir"],
        help="Downstream data directory",
    )
    parser.add_argument(
        "--downstream-model-dir", type=str, default=DOWNSTREAM_CONFIG["model_output_dir"],
        help="Downstream model output directory",
    )

    # Downstream-specific parameters (uses DOWNSTREAM_CONFIG defaults)
    parser.add_argument(
        "--downstream-batch-size", type=int, default=DOWNSTREAM_CONFIG["batch_size"],
        help="Batch size for downstream training",
    )
    parser.add_argument(
        "--downstream-learning-rate", type=float, default=DOWNSTREAM_CONFIG["learning_rate"],
        help="Learning rate for downstream training",
    )
    parser.add_argument(
        "--downstream-hidden-dim", type=int, default=DOWNSTREAM_CONFIG["hidden_dim"],
        help="Hidden dimension for downstream Dense layers",
    )

    args = parser.parse_args()

    # Generate dummy data (uses DUMMY_DATA_CONFIG for all parameters)
    # Uses new folder structure: data/{step}/{train|val|test}/*.parquet
    if args.mode == "generate-dummy":
        logger.info("=" * 80)
        logger.info("üé≤ Generating Dummy Data with New Folder Structure")
        logger.info("=" * 80)
        
        # Pre-training: only train folder (no validation)
        generate_dummy_pretrain_data(
            output_path="data/pretrain/train/dummy_pretrain.parquet",
        )
        
        # Fine-tuning: train + val folders
        generate_dummy_finetune_data(
            output_path="data/finetune/train/dummy_finetune_train.parquet",
            num_samples=DUMMY_DATA_CONFIG["finetune_train_samples"],
        )
        generate_dummy_finetune_data(
            output_path="data/finetune/val/dummy_finetune_val.parquet",
            num_samples=DUMMY_DATA_CONFIG["finetune_val_samples"],
        )
        
        # Downstream: train + val + test folders
        generate_dummy_downstream_data(
            output_path="data/downstream/train/dummy_downstream_train.parquet",
            num_samples=DUMMY_DATA_CONFIG["downstream_train_samples"],
        )
        generate_dummy_downstream_data(
            output_path="data/downstream/val/dummy_downstream_val.parquet",
            num_samples=DUMMY_DATA_CONFIG["downstream_val_samples"],
        )
        generate_dummy_downstream_data(
            output_path="data/downstream/test/dummy_downstream_test.parquet",
            num_samples=DUMMY_DATA_CONFIG.get("downstream_test_samples", 1000),
        )
        
        logger.info("=" * 80)
        logger.info("‚úÖ Dummy data generation complete!")
        logger.info("Folder structure:")
        logger.info("  data/pretrain/train/*.parquet")
        logger.info("  data/finetune/train/*.parquet")
        logger.info("  data/finetune/val/*.parquet")
        logger.info("  data/downstream/train/*.parquet")
        logger.info("  data/downstream/val/*.parquet")
        logger.info("  data/downstream/test/*.parquet")
        logger.info("=" * 80)
        return

    # Build parameters
    pretrain_params = PretrainParams(
        embed_dim=args.embed_dim,
        ff_dim=args.ff_dim,
        nb_head=args.nb_head,
        nb_transformer_blocs=args.nb_transformer_blocs,
        batch_size=args.batch_size,
        epochs=args.epochs,
        learning_rate=args.learning_rate,
        data_dir=args.pretrain_data_dir,
        model_output_dir=args.pretrain_model_dir,
    )

    finetune_params = FinetuneParams(
        embed_dim=args.embed_dim,
        ff_dim=args.ff_dim,
        nb_head=args.nb_head,
        nb_transformer_blocs=args.nb_transformer_blocs,
        batch_size=args.batch_size,
        epochs=args.epochs,
        finetune_learning_rate=args.finetune_learning_rate,
        data_dir=args.finetune_data_dir,
        pretrained_model_path=f"{args.pretrain_model_dir}/hubert_ntxent_model",  # Folder path
        model_output_dir=args.finetune_model_dir,
    )

    downstream_params = DownstreamParams(
        embed_dim=args.embed_dim,
        batch_size=args.downstream_batch_size,
        epochs=args.epochs,
        downstream_learning_rate=args.downstream_learning_rate,
        hidden_dim=args.downstream_hidden_dim,
        data_dir=args.downstream_data_dir,
        finetuned_model_path=f"{args.finetune_model_dir}/hubert_ctr_finetuned",  # Folder path
        model_output_dir=args.downstream_model_dir,
    )

    # Run pipeline
    if args.mode in ("pretrain", "all"):
        pretrainer = HuBertPretrainer(pretrain_params)
        pretrain_path = pretrainer.run()

    if args.mode in ("finetune", "all"):
        finetuner = HuBertFinetuner(finetune_params)
        finetune_path = finetuner.run()

    if args.mode in ("downstream", "all"):
        downstreamer = HuBertDownstream(downstream_params)
        downstream_path = downstreamer.run()

    logger.info("=" * 80)
    logger.info("üéâ HuBERT Experiment Pipeline Complete!")
    logger.info("=" * 80)


if __name__ == "__main__":
    main()

